# -*- coding: utf-8 -*-
"""boston_housing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wusj5eoszFHYak5wKTujBKnQ6jIhfsJi
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

"""# Load dataset
boston_housing  
https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data

Load boston_housing dataset

將data分為training set及 test set，其中training set為404筆，test set為102筆
"""

(x_train,y_train),(x_test,y_test) = tf.keras.datasets.boston_housing.load_data(path='boston_housing.npz', test_split=0.2, seed=113)

"""training set有 404 間房子，有13個屬性

test set有 102 間房子，有13個屬性
"""

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

"""#Data preprocess

將training set作正規化，減去平均值再除上標準差
"""

mean = x_train.mean(axis=0)
x_train -= mean
std = x_train.std(axis=0)
x_train /= std

"""training set也使用訓練資料的平均值及標準差"""

x_test -= mean
x_test /= std

"""#Build the model"""

from keras.layers import Dense

"""建立模型:

model.add(Dense):
* units:fully connected layer的神經元數量
* activation:激活函數

"""

model = tf.keras.models.Sequential()
model.add(Dense(input_shape=(13, ), units=8, activation='relu'))
model.add(Dense(units=8, activation='relu'))
model.add(Dense(units=1))

model.summary()

"""產生FLOPs的function"""

from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph

def get_flops(model):
    concrete = tf.function(lambda inputs: model(inputs))
    concrete_func = concrete.get_concrete_function(
        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])
    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)
    with tf.Graph().as_default() as graph:
        tf.graph_util.import_graph_def(graph_def, name='')
        run_meta = tf.compat.v1.RunMetadata()
        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()
        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd="op", options=opts)
        return flops.total_float_ops

"""顯示FLOPs的數量"""

print("The FLOPs is:{}".format(get_flops(model)) ,flush=True)

"""顯示model的各個layer所代表的意義，以及data size的變化"""

tf.keras.utils.plot_model(model, show_shapes=True)

"""#Compile the model
Configure the learning process with compile() API before training the model. It receives three arguments:

*   An optimizer 
*   A loss function 
*   A list of metrics  

https://www.tensorflow.org/api_docs/python/tf/optimizers  
https://www.tensorflow.org/api_docs/python/tf/keras/losses

編譯: 選擇損失函數、優化方法及成效衡量方式
"""

model.compile(
      loss='mse',
      optimizer='rmsprop',
      metrics=['mae'])

"""#Train the model

將training data對模型做訓練，訓練600次，每個iteration以64筆做計算，並以1成training set當成validation set
"""

Train_history = model.fit(
            x=x_train, 
            y=y_train, 
            batch_size=64,
            epochs=600, 
            validation_split=0.1)

"""#Plot the learning curve

第一張為loss的曲線圖，從圖可以看出training data及validation data的loss逐漸降低

第二張為mae的曲線圖，從圖可以看出training data及validation data的mae逐漸降低，收斂在2點多
"""

plt.title('Loss')
plt.xlabel('epoch')
plt.plot(Train_history.history['loss'], label='Training')
plt.plot(Train_history.history['val_loss'], label='Validation')
plt.legend()
plt.show()

plt.title('mae')
plt.xlabel('epoch')
plt.plot(Train_history.history['mae'], label='Training')
plt.plot(Train_history.history['val_mae'], label='Validation')
plt.legend()
plt.show()

"""#Evaluate

將test data做輸入，用來評估model的好壞
"""

Test_result = model.evaluate(x = x_test, y = y_test, batch_size = x_test.shape[0])
print("Loss on testing set: %f" % Test_result[0])
print("mae on testing set: ", Test_result[1])