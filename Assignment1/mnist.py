# -*- coding: utf-8 -*-
"""mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gUxQJeBqW6Uf3OxMirMkfoOfWkMqk5m5
"""

import tensorflow as tf
import matplotlib.pyplot as plt

"""# Load dataset
MNIST  
https://www.tensorflow.org/api_docs/python/tf/keras/datasets/mnist/load_data
"""

# Load MNIST dataset
# mnist.load_data() method returns tuple of NumPy arrays.
(x_train,y_train),(x_test,y_test) = tf.keras.datasets.mnist.load_data()

print(x_train.shape) #The training set has 60000 images of size 28*28.
print(y_train.shape) #60,000 labels in the training set 
print(x_test.shape) #The testing set has 10000 images of size 28*28.
print(y_test.shape) #10,000 labels in the testing set

# Print the label of the 1000th data in the training set
print(y_train[999])

# Draw the 1000th data
plt.imshow(x_train[999], cmap='gray')

#Take a look what the 1000th data is
#print(x_train[999])

"""# Data preprocess"""

# Change data type from uint8 to float32
# Because we is going to do normalization
x_train = x_train.astype("float32")
x_test = x_test.astype("float32")

# Normalization
x_train = x_train/255
x_test = x_test/255

# Reshape input data from (28, 28) to (28, 28, 1)
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) # x_train.shape:(60000, 28, 28) -> (60000, 28, 28, 1)
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)  # x_test.shape:(60000, 28, 28) -> (60000, 28, 28, 1)

# One-hot encode the labels
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

print(y_train[999])

"""# Build the model


"""

from keras.layers import Dense, Conv2D, AveragePooling2D, MaxPooling2D, Flatten

model = tf.keras.models.Sequential()
# Input shape needs to be specified in the first layer
model.add(Conv2D(input_shape=(28,28,1), filters=3, kernel_size=(5, 5), activation='tanh'))
model.add(AveragePooling2D(2,2))
model.add(Conv2D(filters=2, kernel_size=(5, 5), activation='tanh'))
model.add(AveragePooling2D(2,2))
model.add(Flatten())
model.add(Dense(units=15, activation='tanh'))
model.add(Dense(units=10, activation="softmax"))

# Print the model summary
model.summary()

from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph

#Get Model FLOPs function
def get_flops(model):
    concrete = tf.function(lambda inputs: model(inputs))
    concrete_func = concrete.get_concrete_function(
        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])
    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)
    with tf.Graph().as_default() as graph:
        tf.graph_util.import_graph_def(graph_def, name='')
        run_meta = tf.compat.v1.RunMetadata()
        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()
        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd="op", options=opts)
        return flops.total_float_ops

print("The FLOPs is:{}".format(get_flops(model)) ,flush=True)

#plot the model shape
tf.keras.utils.plot_model(model, show_shapes=True)

"""#Compile the model
Configure the learning process with compile() API before training the model. It receives three arguments:

*   An optimizer 
*   A loss function 
*   A list of metrics  

https://www.tensorflow.org/api_docs/python/tf/optimizers  
https://www.tensorflow.org/api_docs/python/tf/keras/losses
"""

model.compile(
      loss='categorical_crossentropy',
      optimizer='adam',
      metrics=['accuracy'])

"""#Train the model"""

# Feed the input data to this model and start training 
Train_history = model.fit(
            x=x_train, 
            y=y_train, 
            batch_size=64,
            epochs=10, 
            validation_split=0.1)

"""#Plot the learning curve"""

# Plot loss curve
plt.title('Loss')
plt.xlabel('epoch')
plt.plot(Train_history.history['loss'], label='Training')
plt.plot(Train_history.history['val_loss'], label='Validation')
plt.legend()
plt.show()
# Plot accuracy curve
plt.title('Accuracy')
plt.xlabel('epoch')
plt.plot(Train_history.history['accuracy'], label='Training')
plt.plot(Train_history.history['val_accuracy'], label='Validation')
plt.legend()
plt.show()

"""# Evaluate"""

# Evaluate the model on test set
Test_result = model.evaluate(x = x_test, y = y_test, batch_size = x_test.shape[0])
# Print test loss and test accuracy
print("Loss on testing set: %f" % Test_result[0])
print("Accuracy on testing set: %f" % Test_result[1])